{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":77807,"databundleVersionId":8765512,"sourceType":"competition"},{"sourceId":7510622,"sourceType":"datasetVersion","datasetId":4374350}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-01T14:23:49.142194Z","iopub.execute_input":"2024-07-01T14:23:49.142950Z","iopub.status.idle":"2024-07-01T14:23:50.488777Z","shell.execute_reply.started":"2024-07-01T14:23:49.142904Z","shell.execute_reply":"2024-07-01T14:23:50.487391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import  RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error,make_scorer\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\npip install mlxtend\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.metrics import mean_squared_log_error","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:50.491251Z","iopub.execute_input":"2024-07-01T14:23:50.491761Z","iopub.status.idle":"2024-07-01T14:23:54.817841Z","shell.execute_reply.started":"2024-07-01T14:23:50.491730Z","shell.execute_reply":"2024-07-01T14:23:54.816443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Download the data**","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/ml-competition-2024-for-ukrainians/train.csv')\ndf_test = pd.read_csv('/kaggle/input/ml-competition-2024-for-ukrainians/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/ml-competition-2024-for-ukrainians/sample_submission.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:54.819566Z","iopub.execute_input":"2024-07-01T14:23:54.820279Z","iopub.status.idle":"2024-07-01T14:23:57.264436Z","shell.execute_reply.started":"2024-07-01T14:23:54.820191Z","shell.execute_reply":"2024-07-01T14:23:57.263137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:57.268172Z","iopub.execute_input":"2024-07-01T14:23:57.268590Z","iopub.status.idle":"2024-07-01T14:23:57.313931Z","shell.execute_reply.started":"2024-07-01T14:23:57.268556Z","shell.execute_reply":"2024-07-01T14:23:57.312767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What features we have**","metadata":{}},{"cell_type":"markdown","source":"1.Item_Identifier: A unique identifier for each product.\n\n2.Item_Weight: The weight of the product.\n\n3.Item_Fat_Content: The fat content of the product, which typically has values like 'Low Fat', 'Regular', etc.\n\n4.Item_Visibility: The percentage of total display area allocated to this product in the store.\n\n5.Item_Type: The category to which the product belongs, such as 'Dairy', 'Soft Drinks', 'Meat', etc.\n\n6.Item_MRP: The maximum retail price (list price) of the product.\n\n7.Outlet_Identifier: A unique identifier for each store.\n\n8.Outlet_Establishment_Year: The year in which the store was established.\n\n9.Outlet_Size: The size of the store in terms of ground area, usually categorized as 'Small', 'Medium', 'High'.\n\n10.Outlet_Location_Type: The type of city in which the store is located, such as 'Tier 1', 'Tier 2', 'Tier 3'.\n\n11.Outlet_Type: The type of store, such as 'Grocery Store', 'Supermarket Type1', 'Supermarket Type2', 'Supermarket Type3'.\n\n12.Item_Outlet_Sales: The sales of the product in the particular store (target variable).","metadata":{}},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:58.963714Z","iopub.execute_input":"2024-07-01T14:23:58.964540Z","iopub.status.idle":"2024-07-01T14:23:58.980306Z","shell.execute_reply.started":"2024-07-01T14:23:58.964491Z","shell.execute_reply":"2024-07-01T14:23:58.978847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:58.982214Z","iopub.execute_input":"2024-07-01T14:23:58.983483Z","iopub.status.idle":"2024-07-01T14:23:58.997029Z","shell.execute_reply.started":"2024-07-01T14:23:58.983430Z","shell.execute_reply":"2024-07-01T14:23:58.995437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets drop \"id\" column from our test and train data set**","metadata":{}},{"cell_type":"code","source":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(df_train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(df_test.shape))\n\n#Save the 'Id' column\ntrain_ID = df_train['id']\ntest_ID = df_test['id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ndf_train.drop(\"id\", axis = 1, inplace = True)\ndf_test.drop(\"id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(df_train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(df_test.shape))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:59.001825Z","iopub.execute_input":"2024-07-01T14:23:59.002245Z","iopub.status.idle":"2024-07-01T14:23:59.073852Z","shell.execute_reply.started":"2024-07-01T14:23:59.002212Z","shell.execute_reply":"2024-07-01T14:23:59.072561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:59.075181Z","iopub.execute_input":"2024-07-01T14:23:59.075565Z","iopub.status.idle":"2024-07-01T14:23:59.084681Z","shell.execute_reply.started":"2024-07-01T14:23:59.075519Z","shell.execute_reply":"2024-07-01T14:23:59.083363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA ( Exploratory Data Analysis)","metadata":{}},{"cell_type":"markdown","source":"**Lets look to distribution of our target - Item_Outlet_Sales**","metadata":{}},{"cell_type":"code","source":"sns.histplot(df_train['Item_Outlet_Sales'])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:23:59.086383Z","iopub.execute_input":"2024-07-01T14:23:59.086822Z","iopub.status.idle":"2024-07-01T14:24:01.138654Z","shell.execute_reply.started":"2024-07-01T14:23:59.086781Z","shell.execute_reply":"2024-07-01T14:24:01.137030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = df_train['Item_MRP'], y = df_train['Item_Outlet_Sales'])\nplt.ylabel('Item_Outlet_Sales', fontsize=13)\nplt.xlabel('Item_Weight', fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:01.140406Z","iopub.execute_input":"2024-07-01T14:24:01.140904Z","iopub.status.idle":"2024-07-01T14:24:02.256097Z","shell.execute_reply.started":"2024-07-01T14:24:01.140861Z","shell.execute_reply":"2024-07-01T14:24:02.254441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Item_Outlet_Sales'].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:02.257825Z","iopub.execute_input":"2024-07-01T14:24:02.258304Z","iopub.status.idle":"2024-07-01T14:24:02.288723Z","shell.execute_reply.started":"2024-07-01T14:24:02.258259Z","shell.execute_reply":"2024-07-01T14:24:02.287421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Deviate from the Normal Distribution**\n\nWhen we say that the distribution of Item_Outlet_Sales deviates from the normal distribution, it means that the shape of its distribution does not follow the familiar bell curve of a normal distribution. This can have several implications:\n\nAsymmetry: The distribution might be skewed to the left or right rather than being symmetric.\nKurtosis: The distribution might have more or less peakedness compared to a normal distribution.\nOutliers: There could be more outliers than expected in a normal distribution.\nUnderstanding this deviation is crucial because many statistical methods and machine learning algorithms assume normality. If the data deviates significantly from normality, we might need to apply transformations (like logarithmic, square root, etc.) or use non-parametric methods.\n\n# **2. Appreciable Positive Skewness**\nPositive skewness (right-skewed) means that the tail on the right side of the distribution is longer or fatter than the left side. For Item_Outlet_Sales, this implies:\n\nHigher Sales Outliers: There are some stores with much higher sales than the majority.\nMean > Median: The mean of the sales will be greater than the median because of the high-value outliers.\nImplications of positive skewness include:\n\nModel Performance: Some models, especially linear ones, may not perform well if the target variable is highly skewed.\nInterpretation: Statistical measures like the mean can be misleading.\n# **3. Show Peakedness (Kurtosis)**\nPeakedness, or kurtosis, refers to the \"tailedness\" of the distribution. For Item_Outlet_Sales:\n\nHigh Kurtosis (Leptokurtic): This would mean a sharper peak with fatter tails, indicating more data in the tails and fewer in the shoulders.\nLow Kurtosis (Platykurtic): This indicates a flatter peak with thinner tails.\nImplications of kurtosis include:\n\nRisk Assessment: Higher kurtosis might indicate more risk in terms of prediction errors since extreme values (outliers) are more common.\nDistribution Shape: It helps in understanding the concentration of data points around the mean.\n","metadata":{}},{"cell_type":"code","source":"print('Skewness: %f' % df_train['Item_Outlet_Sales'].skew())\nprint('Kurtsis: %f' %df_train['Item_Outlet_Sales'].kurt())","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:02.319059Z","iopub.execute_input":"2024-07-01T14:24:02.319512Z","iopub.status.idle":"2024-07-01T14:24:02.338401Z","shell.execute_reply.started":"2024-07-01T14:24:02.319472Z","shell.execute_reply":"2024-07-01T14:24:02.336927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpretation of Skewness and Kurtosis Values\n**Skewness:**\n0: Perfectly symmetrical.\n\nmore 0: Positively skewed (right-skewed).\n\n<0: Negatively skewed (left-skewed).\n\n**Kurtosis:**\n0: Mesokurtic (normal distribution).\n\nmore 0: Leptokurtic (sharper peak, fatter tails).\n\n<0: Platykurtic (flatter peak, thinner tails).","metadata":{}},{"cell_type":"markdown","source":"Skewness: 3.073422\nThis is a high positive skewness value. Positive skewness means that the right tail of the distribution is longer or fatter than the left. In practical terms:\n\nMost Sales Are Low to Moderate: Most store sales are clustered at lower values, with fewer instances of extremely high sales.\nOutliers: There are a few instances where sales are significantly higher than the majority of the data.\nMean > Median: The mean sales value will be higher than the median due to these few high sales values.\nKurtosis: 28.001507\nThis is a very high kurtosis value. High kurtosis (leptokurtic distribution) means that the distribution has a sharp peak and fat tails. In practical terms:\n\nSharp Peak: Most sales values are very close to the mean value.\nFat Tails: There are more extreme values (outliers) than you would expect in a normal distribution.\nRisk: This indicates higher risk, as extreme values are more likely to occur, which can affect the stability and performance of your model.\nImplications for Modeling\nGiven these characteristics, here's what you might consider doing for better model performance:\n\nTransformations to Normalize Data:\n\nLog Transformation: Applying a log transformation can help in reducing skewness.\nSquare Root Transformation: This can also help in reducing skewness but is less powerful than log transformation.\nBox-Cox Transformation: This can handle both skewness and stabilize variance.\nHandling Outliers:\n\nCap and Floor: You can cap the extreme values at a certain threshold to reduce their impact.\nOutlier Removal: In some cases, removing extreme outliers can help.\nRobust Models:\n\nTree-Based Models: Models like Random Forests, Gradient Boosting Machines (GBM), and XGBoost are less sensitive to skewed distributions and outliers.\nRegularization: Using regularized models like Ridge or Lasso regression can help in dealing with skewed data.","metadata":{}},{"cell_type":"markdown","source":"# Log-transformation of the target variable","metadata":{}},{"cell_type":"code","source":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"Item_Outlet_Sales\"] = np.log1p(df_train[\"Item_Outlet_Sales\"])\n\n#Check the new distribution \nsns.distplot(df_train['Item_Outlet_Sales'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['Item_Outlet_Sales'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Sales distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['Item_Outlet_Sales'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:02.339905Z","iopub.execute_input":"2024-07-01T14:24:02.340371Z","iopub.status.idle":"2024-07-01T14:24:05.970895Z","shell.execute_reply.started":"2024-07-01T14:24:02.340308Z","shell.execute_reply":"2024-07-01T14:24:05.969439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding missing values","metadata":{}},{"cell_type":"code","source":"df_train.apply(lambda x: sum(x.isnull()))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:06.001525Z","iopub.execute_input":"2024-07-01T14:24:06.002046Z","iopub.status.idle":"2024-07-01T14:24:06.919175Z","shell.execute_reply.started":"2024-07-01T14:24:06.001901Z","shell.execute_reply":"2024-07-01T14:24:06.917734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.apply(lambda x : len(x.unique()))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:06.920668Z","iopub.execute_input":"2024-07-01T14:24:06.921148Z","iopub.status.idle":"2024-07-01T14:24:07.219749Z","shell.execute_reply.started":"2024-07-01T14:24:06.921105Z","shell.execute_reply":"2024-07-01T14:24:07.218518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This tells us that there are 1559 products and 10 outlets/stores and no missing values","metadata":{}},{"cell_type":"markdown","source":"**Lets look at boxplots for different features**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,9))\n\nplt.subplot(311)\nsns.boxplot(x='Outlet_Size', y='Item_Outlet_Sales', data=df_train, palette=\"Set1\")\n\nplt.subplot(312)\nsns.boxplot(x='Outlet_Location_Type', y='Item_Outlet_Sales', data=df_train, palette=\"Set1\")\n\nplt.subplot(313)\nsns.boxplot(x='Outlet_Type', y='Item_Outlet_Sales', data=df_train, palette=\"Set1\")\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.4,top = 1.5)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:07.221753Z","iopub.execute_input":"2024-07-01T14:24:07.222236Z","iopub.status.idle":"2024-07-01T14:24:08.805401Z","shell.execute_reply.started":"2024-07-01T14:24:07.222194Z","shell.execute_reply":"2024-07-01T14:24:08.803703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (14,9))\n\nplt.subplot(211)\nax = sns.boxplot(x='Outlet_Identifier', y='Item_Outlet_Sales', data=df_train, palette=\"Set1\")\nax.set_title(\"Outlet_Identifier vs. Item_Outlet_Sales\", fontsize=15)\nax.set_xlabel(\"\", fontsize=12)\nax.set_ylabel(\"Item_Outlet_Sales\", fontsize=12)\n\nplt.subplot(212)\nax = sns.boxplot(x='Item_Type', y='Item_Outlet_Sales', data=df_train, palette=\"Set1\")\nax.set_title(\"Item_Type vs. Item_Outlet_Sales\", fontsize=15)\nax.set_xlabel(\"\", fontsize=12)\nax.set_ylabel(\"Item_Outlet_Sales\", fontsize=12)\n\nplt.subplots_adjust(hspace = 0.9, top = 0.9)\nplt.setp(ax.get_xticklabels(), rotation=45)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:08.807372Z","iopub.execute_input":"2024-07-01T14:24:08.807861Z","iopub.status.idle":"2024-07-01T14:24:10.385261Z","shell.execute_reply.started":"2024-07-01T14:24:08.807817Z","shell.execute_reply":"2024-07-01T14:24:10.383944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Item_Visibility'].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:10.387168Z","iopub.execute_input":"2024-07-01T14:24:10.387632Z","iopub.status.idle":"2024-07-01T14:24:10.417962Z","shell.execute_reply.started":"2024-07-01T14:24:10.387590Z","shell.execute_reply":"2024-07-01T14:24:10.416659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There is the problem. Item_visibility can not be zero.**","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing steps\n","metadata":{}},{"cell_type":"markdown","source":"Drop some columns that are not important. ( I experimented with different columns)","metadata":{}},{"cell_type":"code","source":"df_train.drop([\"Outlet_Identifier\", \"Outlet_Establishment_Year\"],axis=1,inplace=True)\ndf_test.drop([\"Outlet_Identifier\", \"Outlet_Establishment_Year\"],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:10.431331Z","iopub.execute_input":"2024-07-01T14:24:10.431721Z","iopub.status.idle":"2024-07-01T14:24:10.499440Z","shell.execute_reply.started":"2024-07-01T14:24:10.431689Z","shell.execute_reply":"2024-07-01T14:24:10.498096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fill missing values in Item_Visibility with mean value.","metadata":{}},{"cell_type":"code","source":"#Average visibility of the product\nvisibility_avg = df_train.pivot_table(values = 'Item_Visibility', index='Item_Identifier')\n\n#Impute 0 values with mean visibility of that product:\nmissing_values = (df_train['Item_Visibility'] ==0)\n\nprint ('Number of 0 values initially: %d'%sum(missing_values))\ndf_train.loc[missing_values,'Item_Visibility'] = df_train.loc[missing_values,'Item_Identifier'].apply(lambda x: visibility_avg.at[x, 'Item_Visibility'])\nprint ('Number of 0 values after modification: %d'%sum(df_train['Item_Visibility'] == 0))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:10.500935Z","iopub.execute_input":"2024-07-01T14:24:10.501406Z","iopub.status.idle":"2024-07-01T14:24:11.328696Z","shell.execute_reply.started":"2024-07-01T14:24:10.501362Z","shell.execute_reply":"2024-07-01T14:24:11.327364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Average visibility of the product\nvisibility_avg = df_test.pivot_table(values = 'Item_Visibility', index='Item_Identifier')\n\n#Impute 0 values with mean visibility of that product:\nmissing_values = (df_test['Item_Visibility'] ==0)\n\nprint ('Number of 0 values initially: %d'%sum(missing_values))\ndf_test.loc[missing_values,'Item_Visibility'] = df_test.loc[missing_values,'Item_Identifier'].apply(lambda x: visibility_avg.at[x, 'Item_Visibility'])\nprint ('Number of 0 values after modification: %d'%sum(df_test['Item_Visibility'] == 0))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:11.335700Z","iopub.execute_input":"2024-07-01T14:24:11.336083Z","iopub.status.idle":"2024-07-01T14:24:11.900269Z","shell.execute_reply.started":"2024-07-01T14:24:11.336054Z","shell.execute_reply":"2024-07-01T14:24:11.898834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we will clean the `Item_Fat_Content` column by replacing inconsistent category names with more uniform ones. This helps in ensuring that the data is consistent and ready for further analysis or modeling.","metadata":{}},{"cell_type":"code","source":"print('Original Categories:')\nprint(df_train['Item_Fat_Content'].value_counts())\n\nprint('\\nModified Categories:')\ndf_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].replace({'LF':'Low Fat',\n                                                             'reg':'Regular',\n                                                             'low fat':'Low Fat'})\nprint(df_train['Item_Fat_Content'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:11.901723Z","iopub.execute_input":"2024-07-01T14:24:11.902198Z","iopub.status.idle":"2024-07-01T14:24:12.197191Z","shell.execute_reply.started":"2024-07-01T14:24:11.902157Z","shell.execute_reply":"2024-07-01T14:24:12.195661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Original Categories:')\nprint(df_test['Item_Fat_Content'].value_counts())\n\nprint('\\nModified Categories:')\ndf_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].replace({'LF':'Low Fat',\n                                                             'reg':'Regular',\n                                                             'low fat':'Low Fat'})\nprint(df_test['Item_Fat_Content'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:12.199436Z","iopub.execute_input":"2024-07-01T14:24:12.199819Z","iopub.status.idle":"2024-07-01T14:24:12.393522Z","shell.execute_reply.started":"2024-07-01T14:24:12.199788Z","shell.execute_reply":"2024-07-01T14:24:12.392054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we will encode the `Outlet_Size` and `Outlet_Location_Type` column by mapping its categorical values to numerical values. This transformation helps in converting categorical data into a format that can be used for machine learning algorithms.","metadata":{}},{"cell_type":"code","source":"df_train['Outlet_Size'] = df_train['Outlet_Size'].map({'Small'  : 1,\n                                                 'Medium' : 2,\n                                                 'High'   : 3\n                                                 }).astype(int)\n\ndf_test['Outlet_Size'] = df_test['Outlet_Size'].map({'Small'  : 1,\n                                               'Medium' : 2,\n                                               'High'   : 3\n                                              }).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:12.418057Z","iopub.execute_input":"2024-07-01T14:24:12.418585Z","iopub.status.idle":"2024-07-01T14:24:12.527638Z","shell.execute_reply.started":"2024-07-01T14:24:12.418543Z","shell.execute_reply":"2024-07-01T14:24:12.526285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outlet_Location_Type feature encoding by getting the last character and converting to int type\n\ndf_train['Outlet_Location_Type'] = df_train['Outlet_Location_Type'].map({'Tier 1': 1,\n                                                                         'Tier 2': 2,\n                                                                         'Tier 3': 3\n                                                                        }).astype(int)\ndf_test['Outlet_Location_Type'] = df_test['Outlet_Location_Type'].map({'Tier 1': 1,\n                                                                        'Tier 2': 2,\n                                                                        'Tier 3': 3\n                                                                        }).astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:12.770744Z","iopub.execute_input":"2024-07-01T14:24:12.771217Z","iopub.status.idle":"2024-07-01T14:24:12.859112Z","shell.execute_reply.started":"2024-07-01T14:24:12.771173Z","shell.execute_reply":"2024-07-01T14:24:12.857268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Item_Identifier']","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:12.861003Z","iopub.execute_input":"2024-07-01T14:24:12.861526Z","iopub.status.idle":"2024-07-01T14:24:12.873672Z","shell.execute_reply.started":"2024-07-01T14:24:12.861468Z","shell.execute_reply":"2024-07-01T14:24:12.872321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Item_Type feature, there are 16 categories but when we look closely to Item_Identifier_Categories, it has first two characters defining the item type, these are:\n\n\n**FD for probably Food;**\n\n**DR for probably Drinks;**\n\n**NC for probably Non-Consumables.**\n\n\nSo we'll drop the Item_Identifier feature and create a new column containing these categories.","metadata":{}},{"cell_type":"code","source":"df_train['Item_Identifier_Categories'] = df_train['Item_Identifier'].str[0:2] #.astype(int)\ndf_test['Item_Identifier_Categories']  = df_test['Item_Identifier'].str[0:2]\n\nsns.countplot(x=df_train['Item_Identifier_Categories'])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:12.875621Z","iopub.execute_input":"2024-07-01T14:24:12.876059Z","iopub.status.idle":"2024-07-01T14:24:13.919979Z","shell.execute_reply.started":"2024-07-01T14:24:12.876003Z","shell.execute_reply":"2024-07-01T14:24:13.918430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's drop useless columns\ndf_train.drop(labels=['Item_Identifier'], axis=1, inplace=True)\ndf_test.drop(labels=['Item_Identifier'],  axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:13.921526Z","iopub.execute_input":"2024-07-01T14:24:13.921925Z","iopub.status.idle":"2024-07-01T14:24:13.981598Z","shell.execute_reply.started":"2024-07-01T14:24:13.921892Z","shell.execute_reply":"2024-07-01T14:24:13.980311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split dataset to train and valid","metadata":{}},{"cell_type":"code","source":"X = df_train.drop('Item_Outlet_Sales', axis =1)\ny = df_train['Item_Outlet_Sales']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.05, random_state =42)\n\nprint(f\"Training features shape: {X_train.shape}\")\nprint(f\"Validation features shape: {X_valid.shape}\")\nprint(f\"Training target shape: {y_train.shape}\")\nprint(f\"Validation target shape: {y_valid.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:14.022638Z","iopub.execute_input":"2024-07-01T14:24:14.023008Z","iopub.status.idle":"2024-07-01T14:24:14.398227Z","shell.execute_reply.started":"2024-07-01T14:24:14.022975Z","shell.execute_reply":"2024-07-01T14:24:14.397158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numerical and One-Hot Encoding of Categorical variables","metadata":{}},{"cell_type":"code","source":"# Get list of categorical variables\ns = (df_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:14.399762Z","iopub.execute_input":"2024-07-01T14:24:14.400211Z","iopub.status.idle":"2024-07-01T14:24:14.408628Z","shell.execute_reply.started":"2024-07-01T14:24:14.400168Z","shell.execute_reply":"2024-07-01T14:24:14.407333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onehot_encoder = OneHotEncoder(sparse = False)\n\nX_train_encoded = pd.DataFrame(onehot_encoder.fit_transform(X_train[object_cols]))\n\nX_valid_encoded = pd.DataFrame(onehot_encoder.transform(X_valid[object_cols]))\n\nX_test_encoded = pd.DataFrame(onehot_encoder.transform(df_test[object_cols]))\n\n# Add back the encoded column names\nX_train_encoded.columns = onehot_encoder.get_feature_names_out(object_cols)\nX_valid_encoded.columns = onehot_encoder.get_feature_names_out(object_cols)\nX_test_encoded.columns = onehot_encoder.get_feature_names_out(object_cols)\n\n# Reset index to avoid issues during concatenation\nX_train_encoded.reset_index(drop=True, inplace=True)\nX_valid_encoded.reset_index(drop=True, inplace=True)\nX_test_encoded.reset_index(drop=True, inplace=True)\nX_train.reset_index(drop=True, inplace=True)\nX_valid.reset_index(drop=True, inplace=True)\ndf_test.reset_index(drop=True, inplace=True)\n\n# Drop original categorical columns from X_train and X_valid\nX_train = X_train.drop(object_cols, axis=1)\nX_valid = X_valid.drop(object_cols, axis=1)\ndf_test = df_test.drop(object_cols, axis=1)\n\n# Concatenate the encoded columns back to the original dataframes\nX_train = pd.concat([X_train, X_train_encoded], axis=1)\nX_valid = pd.concat([X_valid, X_valid_encoded], axis=1)\nX_test = pd.concat([df_test, X_test_encoded], axis=1)\n\nprint(f\"Encoded training features shape: {X_train.shape}\")\nprint(f\"Encoded validation features shape: {X_valid.shape}\")\nprint(f\"Encoded test features shape: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:14.410078Z","iopub.execute_input":"2024-07-01T14:24:14.410536Z","iopub.status.idle":"2024-07-01T14:24:16.020092Z","shell.execute_reply.started":"2024-07-01T14:24:14.410497Z","shell.execute_reply":"2024-07-01T14:24:16.018608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**scalar**","metadata":{}},{"cell_type":"code","source":"\n\nnumerical_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n\n# Separate the numerical columns from the rest of the features\nX_train_num = X_train[numerical_cols]\nX_valid_num = X_valid[numerical_cols]\nX_test_num = X_test[numerical_cols]\n\n# Initialize the scaler\nscaler = RobustScaler()\n\n# Fit the scaler on the training data and transform the training, validation, and test data\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_num), columns=numerical_cols)\nX_valid_scaled = pd.DataFrame(scaler.transform(X_valid_num), columns=numerical_cols)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test_num), columns=numerical_cols)\n\n# Drop the original numerical columns from the datasets\nX_train.drop(numerical_cols, axis=1, inplace=True)\nX_valid.drop(numerical_cols, axis=1, inplace=True)\nX_test.drop(numerical_cols, axis=1, inplace=True)\n\n# Concatenate the scaled numerical columns back to the datasets\nX_train = pd.concat([X_train.reset_index(drop=True), X_train_scaled.reset_index(drop=True)], axis=1)\nX_valid = pd.concat([X_valid.reset_index(drop=True), X_valid_scaled.reset_index(drop=True)], axis=1)\nX_test = pd.concat([X_test.reset_index(drop=True), X_test_scaled.reset_index(drop=True)], axis=1)\n\nprint(f\"Scaled training features shape: {X_train.shape}\")\nprint(f\"Scaled validation features shape: {X_valid.shape}\")\nprint(f\"Scaled test features shape: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:16.021799Z","iopub.execute_input":"2024-07-01T14:24:16.022284Z","iopub.status.idle":"2024-07-01T14:24:16.275215Z","shell.execute_reply.started":"2024-07-01T14:24:16.022240Z","shell.execute_reply":"2024-07-01T14:24:16.273849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing spaces in column names with underscores\nX_train.columns = [col.replace(' ', '_') for col in X_train.columns]\nX_valid.columns = [col.replace(' ', '_') for col in X_valid.columns]\nX_test.columns = [col.replace(' ', '_') for col in X_test.columns]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:16.277173Z","iopub.execute_input":"2024-07-01T14:24:16.277680Z","iopub.status.idle":"2024-07-01T14:24:16.286329Z","shell.execute_reply.started":"2024-07-01T14:24:16.277636Z","shell.execute_reply":"2024-07-01T14:24:16.285026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model. Training","metadata":{}},{"cell_type":"code","source":"#Metric\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\n\n\n# Define base models ( All parameters were found using GridSearchCV)\nlasso = Lasso(alpha=0.0001, fit_intercept = True)\nrandomforest = RandomForestRegressor(n_estimators = 200, min_samples_leaf = 100, max_depth = None, n_jobs = -1, random_state = 42)\nxgboost = XGBRegressor(learning_rate=0.1, max_depth=5, n_estimators=100)\nlightgbm = LGBMRegressor(\n    learning_rate=0.05,\n    max_depth=30,\n    n_estimators=1000,\n    num_leaves=100,\n    min_child_samples=50,\n    metric='rmsle',\n    boosting_type='gbdt',\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=0.1,\n    random_state=42\n)\nridge = Ridge(alpha=3, fit_intercept = True, solver = 'sparse_cg')\n# Define meta-model\nmeta_model = XGBRegressor(learning_rate=0.1, max_depth=5, n_estimators=100)\n\n# Initialize the StackingCVRegressor\nstack_gen = StackingCVRegressor(regressors=(ridge,lasso,randomforest, xgboost, lightgbm),\n                                meta_regressor=meta_model,\n                                use_features_in_secondary=True,\n                                cv=5)\n\nstack_gen.fit(X_train, y_train)\n\n\ny_train_pred = stack_gen.predict(X_train)\ny_valid_pred = stack_gen.predict(X_valid)\ny_test_pred = stack_gen.predict(X_test)\n\n# Ensure predictions are non-negative\ny_train_pred = np.maximum(y_train_pred, 0)\ny_valid_pred = np.maximum(y_valid_pred, 0)\ny_test_pred = np.maximum(y_test_pred, 0)\n\n# Inverse transformation of predictions to the original scale\ny_train_pred_original = np.expm1(y_train_pred)\ny_valid_pred_original = np.expm1(y_valid_pred)\n\ntrain_rmsle = rmsle(np.expm1(y_train), y_train_pred_original)\nvalid_rmsle = rmsle(np.expm1(y_valid), y_valid_pred_original)\n\nprint(f'Training RMSLE: {train_rmsle}')\nprint(f'Validation RMSLE: {valid_rmsle}')","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:24:35.319074Z","iopub.execute_input":"2024-07-01T14:24:35.319988Z","iopub.status.idle":"2024-07-01T14:39:43.246883Z","shell.execute_reply.started":"2024-07-01T14:24:35.319941Z","shell.execute_reply":"2024-07-01T14:39:43.245425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"y_test_pred_original_scale = np.expm1(y_test_pred)\nsample_submission['Item_Outlet_Sales'] = y_test_pred_original_scale\nsubmission = sample_submission\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:39:43.248735Z","iopub.execute_input":"2024-07-01T14:39:43.249136Z","iopub.status.idle":"2024-07-01T14:39:43.259639Z","shell.execute_reply.started":"2024-07-01T14:39:43.249100Z","shell.execute_reply":"2024-07-01T14:39:43.258426Z"},"trusted":true},"execution_count":null,"outputs":[]}]}